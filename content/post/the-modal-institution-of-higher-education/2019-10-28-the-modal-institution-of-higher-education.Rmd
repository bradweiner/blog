---
title: The Modal Institution of Higher Education
author: Brad
date: '2019-10-28'
slug: modal-institution
categories:
  - highered
  - R
tags:
  - datascience
  - highered
  - IPEDS
subtitle: ''
summary: ''
authors: []
lastmod: '2019-10-28T20:49:48-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---



I have enjoyed NPR's [Planet Money](https://www.npr.org/podcasts/510289/planet-money) podcast for many years. They always have an interesting perspective on matters foreign and domestic; macro and micro; trivial and critical. It's also a space that doesn't shy away from wonky, data-filled policy debates.

A recent episode, [_The Modal American_](https://www.npr.org/2019/08/28/755191639/episode-936-the-modal-american), talked listeners through a full analytic pipeline including the research question, an explanation of the methodology and the results. 

Planet Money's specific research question was whether they could aggregate [IPUMS](https://ipums.org/) data (Go Gophers!) to find the "most typical" American using the *mode* as their measure of central tendancy. 

To do this, they conscripted the estimiable NYT reporter Ben Casselman, who generously shared his [code repo](https://github.com/BenCasselman/planet_money) so anyone could replicate or repurpose the analysis.

<center>
```{r echo=FALSE}
blogdown::shortcode('tweet', '1168869571497025536')
```
</center>

Finding the modal anything is interesting, but as someone who likes to think about education policy, I was happy to find that Isabella Velásquez used the methodology to look for the [Modal School District](https://ivelasq.rbind.io/blog/modal-district/). 

Buiding on her work, I thought it would be interesting to aggregate IPEDS data to find the "Modal Institution of Higher Education."

###Who Cares?

Averages provide a shortcut for understanding what a "typical" thing (i.e. human height, widget, economy, giraffe age, test score, price of tacos, etc.) might be. As we learned from Planet Money, our perceptions of what we think of as the "average American" might be inaccurate.

If journalists or policy makers focus on an inaccurate version of "typical" then perceptions of how to improve systems might begin from an inaccurate baseline

###American Institutions of Higher Education

Let me illustrate why this is a problem.

Imagine a "typical" college or university. In your mind you probably have a four-year, highly-selective, medium-sized, private university on the East Coast. Ivy grows on Gothic buildings. Professors wear tweed while lecturing in front of equation-filled chalkboards while earnest students take notes. Frisbees are absolutely everywhere.

This perception is, unfortunately, what drives the conversation about American Higher Education. Think about the recent [Operation Varsity Blues](https://en.wikipedia.org/wiki/2019_college_admissions_bribery_scandal) incident. 

To be clear, there was legitimate corruption perpetrated by bad actors looking to exploit a system that is largely built on trust. But was it, as Fareed Zakaria presented, a ["crisis"](http://cnnpressroom.blogs.cnn.com/2019/10/02/cnns-fareed-zakaria-examines-elite-college-admissions-from-the-inside/)?

If an "average" college of university is the one described above, then maybe so. But if typical describes something else, then perhaps we're missing much more interesting or important stories about higher education in America.

So what is a "modal" college or university in the United States? 

Let's find out.

###Analysis



####Data Pull

Finding the "modal institution" will depend entirely on what features are selected.  Rather than using features that might be influenced by the institution, I opted for institutional characteristics which are largely determined by funding mechanisms, student populations, degrees awarded, and location. Furthermore, I opted to utilize standardized, industry accepted classifications and categories to avoid subjective binning problems. 

Using the Integrated Postsecondary Education Data Systems (IPEDS) [web interface](https://nces.ed.gov/ipeds/use-the-data), I selected the universe of cases (n =  6,857) along with the following features:



<table>
 <thead>
  <tr>
   <th style="text-align:left;"> var_name </th>
   <th style="text-align:left;"> var_desc </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> unit_id </td>
   <td style="text-align:left;"> IPEDS UNITID </td>
  </tr>
  <tr>
   <td style="text-align:left;"> institution_name </td>
   <td style="text-align:left;"> Institution Name </td>
  </tr>
  <tr>
   <td style="text-align:left;"> opeflag_hd2018 </td>
   <td style="text-align:left;"> Title IV Flag </td>
  </tr>
  <tr>
   <td style="text-align:left;"> stabbr_hd2018 </td>
   <td style="text-align:left;"> State </td>
  </tr>
  <tr>
   <td style="text-align:left;"> sector_hd2018 </td>
   <td style="text-align:left;"> Sector </td>
  </tr>
  <tr>
   <td style="text-align:left;"> iclevel_hd2018 </td>
   <td style="text-align:left;"> Degree Levels Offered </td>
  </tr>
  <tr>
   <td style="text-align:left;"> control_hd2018 </td>
   <td style="text-align:left;"> Institution Funding Control </td>
  </tr>
  <tr>
   <td style="text-align:left;"> deggrant_hd2018 </td>
   <td style="text-align:left;"> Degree Granting Flag </td>
  </tr>
  <tr>
   <td style="text-align:left;"> instcat_hd2018 </td>
   <td style="text-align:left;"> Degree Granting Category </td>
  </tr>
  <tr>
   <td style="text-align:left;"> instsize_hd2018 </td>
   <td style="text-align:left;"> Enrollment Size Category </td>
  </tr>
  <tr>
   <td style="text-align:left;"> c18basic_hd2018 </td>
   <td style="text-align:left;"> Carnegie Classification </td>
  </tr>
  <tr>
   <td style="text-align:left;"> obereg_hd2018 </td>
   <td style="text-align:left;"> Region Category </td>
  </tr>
</tbody>
</table>

####Data Import and Cleaning

```{r,eval=TRUE,echo=TRUE,message=FALSE}
# load libraries

library(tidyverse)
library(magrittr)
library(janitor)
library(dataMeta)
library(here)
library(kableExtra)
```

```{r,eval=TRUE}
# read in raw data files from IPEDS. 

#file 1 is the raw data normalized
d <- read.csv(here("content/post/the-modal-institution-of-higher-education/data/","data_table.csv"),
              stringsAsFactors = F)

#file 2 is the data dictionary from NCES/IPEDS
e <- read.csv(here("content/post/the-modal-institution-of-higher-education/data/","ValueLabels_10-11-2019---735.csv"),
              stringsAsFactors = F)
```

I always like to take a quick look at the data, just to see what I am working with. As you can see below, the variable names have been normalized with digits or codes rather than useful, human-readable labels. 

The data dictionary is in a long format. This means that each case in the data set represents a factor level for one of the features. In order to append on to the primary data frame, the variable names require conversion into a wide format.

```{r,eval=TRUE}
#clean column names

d <- clean_names(d,case = "snake")
glimpse(d)

e <- clean_names(e,case = "snake")
glimpse(e)
```

Although I could force the joins to match on different variable names, it is easier to lean on the automatic matching functionality in the Tidyverse. In order to leverage this, the variable names in both files required some standardizing. Below is that process writen in my odd dialect of TidyBase and in direct contravention of DRY regulations.

```{r,eval=TRUE}

#remove the year information after the underscore in the variable names
names(d) <- gsub("_[^_]+$","",names(d))

#match those names in the description file

names(e)
table(e$variable_name)

e$variable_name %<>% tolower(.)
e$variable_name <- gsub("hd2018",replacement = "", e$variable_name)
e$variable_name <- gsub("\\(",replacement = "", e$variable_name)
e$variable_name <- gsub("\\)",replacement = "", e$variable_name)
e$variable_name <- gsub(" ",replacement = "", e$variable_name)

table(e$variable_name)

```

Next we're going to spread out the data dictionary from long to wide. I should probably admit that these transformations have always been conceptually confusing for me. Melt + cast, gather + spread, or  pivot_longer/pivot_wider have always been exercises in trial + error

```{r,eval=TRUE}
# spread data dictionary from long to wide

e <- spread(e,key = variable_name,value = value_label)
glimpse(e)
```

```{r,eval=TRUE}
# convert all numeric/integer variables to characters to facilitate joins

e <- e %>%
        mutate_if(is.integer,as.character) 

d <- d %>%
        mutate_if(is.integer,as.character)
```

The last data cleaning step is merging the data file with the data dictionary labels. There are many different ways to achieve this. I went with a for a loop that pulls each value, renames it, and appends.

```{r,eval=TRUE,message=FALSE,warning=FALSE}

#select variable names into vector
varnames <- names(e)
varnamestest <- varnames[2:length(varnames)]

#begin for loop

#For each of the variables in the data dictionary: 
# - select the variable and the value
# - remove NAs
# - rename the value to avoid overwriting
# - paste _val on to the variable name
# - append to date frame
# - repeat

for(i in varnamestest){
        
        temp_dict <- e %>%
                select(i,value) %>%
                na.omit() %>%
                rename(.,newval = value)
        
        names(temp_dict) <- c(paste0(i,"_val"),i)
        
        join_df <- left_join(d,temp_dict)
        
        d <- join_df
        
       
}

glimpse(d)

```

Aside from a few weird character returns, everything is looking good. Now on to the aggregations and counts. 

####Aggregations

The final step is counting the number of institutions within each combination of these bins. Initially, I was planning to copy Ben Cassleman's [code](https://github.com/BenCasselman/planet_money/blob/master/final_analysis.R) but opted to use the updated version on Isabella Velásquez's ["Modal School District"](https://ivelasq.rbind.io/blog/modal-district/) instead.


```{r,eval=TRUE}

#use Casselman/Velásquez code to find modal IHE

d.agg <- d %>%
        count(c18basic_val,
              control_val,
              deggrant_val,
              iclevel_val,
              instcat_val,
              instsize_val,
              obereg_val,
              opeflag_val,
              sector_val,
              stabbr_val,
              sort = TRUE) %>%
        slice(1:3)
        
glimpse(d.agg)
```


####Results

```{r,eval=TRUE}
kable(d.agg) %>%
        kable_styling(font_size = 9)
```

A quick look at the table reveals that several of the top spots differ only by geography. It's mildly interesting that so many of these institutions are in Californa, Texas, or Florida, but given the population sizes there, this seems overly granular. Removing both the state and regional aggregations produces this:

```{r,eval=TRUE,echo=FALSE}

#use Casselman/Velásquez code to find modal IHE

d.agg <- d %>%
        count(c18basic_val,
              control_val,
              deggrant_val,
              iclevel_val,
              instcat_val,
              instsize_val,
              opeflag_val,
              sector_val,
              sort = TRUE) %>%
        slice(1:3)
```


```{r,eval=TRUE}
kable(d.agg) %>%
        kable_styling(font_size = 9)
```


With this higher-level aggregation, it appears as though the modal institution of Higher Education is/are:

* Private for-profit
* Nondegree-granting
* Associate's level
* Under 1,000 enrollment
* Receiving Title IV funding

There are 1,452 of these institutions comprising a sizable 21.3% of the total universe.

What, might you ask is an example of one of the modal institution? To find out, I will append the counts back to the original data set, and randomly select one for a closer zoom.

```{r,eval=TRUE,message=FALSE}
set.seed(8675309)

modal <- left_join(d,d.agg) %>%
        filter(n == max(n,na.rm = T)) %>%
        select(unit,institution,13:23) %>%
        sample_n(size = 1)
```

```{r,eval=TRUE}
kable(modal) %>%
        kable_styling(font_size = 9)
```


####The Modal Institution Is...

The California Barber and Beauty College.

This school, and 1,451 schools like it are private, for-profit institutions that qualify for federal Title IV funding which allows schools to receive federal aid dollars from students who qualify. 

There is bound to be variation in the types of schools that fit into this modal bin, but it's worth a quick look at CB&B. 

California requires licensure for cosmetologists and barbers. Assuming Californians enjoy haircuts and makeup, this seems like a reasonable explaination for this institution's "modal-ness" on the demand side. 

On the supply side, schools like the CB&B keep costs low by running operations from small storefronts or malls. Without tentured faculty, physical plant, hospitals, or football teams, the barriers for entry are comparatively low. 

Furthermore, Certificates can be awarded in 44 weeks by anyone who 16 or older and has a diploma or GED. This is presumably a broader pool of prospective students than "Hollywood Typical" institutions that require two to four years to earn a degree. 

The modal-ness might also be explained by the nature of the training itself.  Barbering and cosmotology require hands-on training and frequent practice, both which lend themselves to small, localized schools with individualized attention. For this reason the modal student, is probably very different from the modal institution.


####Acknowledgements

I am far from the first to point out that the colleges in the movies are not the typical American institution. Since this is something like universal knowledge among higher ed wonks,  I won't be able to cite all of the places this argument is found. But a few of note are:

1. [Shut Up About Harvard](https://fivethirtyeight.com/features/shut-up-about-harvard/)

2. [Higher Ed Data Stories](https://highereddatastories.blogspot.com/2019/03/varsity-blues-and-real-admissions-data.html)

Special thanks to [Isabella Velásquez](https://twitter.com/ivelasq3) and [Ben Casselman](https://twitter.com/bencasselman?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor) for their code contributions and shoulders to stand on.

Of course, thanks to [Planet Money](https://www.npr.org/podcasts/510289/planet-money/) for continuing to create such awesome content nearly 1,000 episodes in.

Errors and interpretations are the author's alone. Please submit edits or errors to Github.








 

<!-- <center> -->
<!-- ```{r, echo=FALSE} -->
<!-- blogdown::shortcode("youtube", "O5J2_EI7go4") -->
<!-- ``` -->
<!-- </center> -->








 










